{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNHZsHW8UK4W"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4f31arDhQ_X"
      },
      "outputs": [],
      "source": [
        "''' Importing libraries '''\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import csv\n",
        "from tensorflow import keras\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model, Sequential, load_model, clone_model\n",
        "from keras.layers import Dense\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import shuffle\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "\n",
        "%matplotlib notebook\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3SUuNcQCpC"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "        # apply sin to even index in the array\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # apply cos to odd index in the array\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_qTGE-IQIC5"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model, use_bias=False)\n",
        "        self.wk = tf.keras.layers.Dense(d_model, use_bias=True)\n",
        "        self.wv = tf.keras.layers.Dense(d_model, use_bias=True)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask, return_attention=False):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        if return_attention == False:\n",
        "          q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "          k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "          v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        if return_attention:\n",
        "          return scaled_attention, attention_weights\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention,\n",
        "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xxh_GWnQLNo"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv34lVhTQO-R"
      },
      "outputs": [],
      "source": [
        "class AttentionWithContext(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Attention operation, with a context/query vector, for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "        \"Hierarchical Attention Networks for Document Classification\"\n",
        "        by using a context vector to assist the attention\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.l\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(AttentionWithContext())\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=False):\n",
        "        super(AttentionWithContext, self).__init__()\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
        "        self.u_constraint = tf.keras.constraints.get(u_constraint)\n",
        "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = tf.tensordot(x, self.W, axes=1)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = tf.keras.activations.tanh(uit)\n",
        "        ait = tf.tensordot(uit, self.u, axes=1)\n",
        "\n",
        "        a = tf.math.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= tf.cast(mask, tf.keras.backend.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= tf.cast(tf.keras.backend.sum(a, axis=1, keepdims=True) + tf.keras.backend.epsilon(),\n",
        "                     tf.keras.backend.floatx())\n",
        "\n",
        "        a = tf.keras.backend.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        result = tf.keras.backend.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return result, a\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return tf.TensorShape([input_shape[0].value, input_shape[-1].value],\n",
        "                                  [input_shape[0].value, input_shape[1].value])\n",
        "        else:\n",
        "            return tf.TensorShape([input_shape[0].value, input_shape[-1].value])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63h3FLACQSL5"
      },
      "outputs": [],
      "source": [
        "def create_model(n_timesteps, n_features, n_outputs, _dff=512, d_model=128, nh=4, dropout_rate=0.2, use_pe=True):\n",
        "    inputs = tf.keras.layers.Input(shape=(n_timesteps, n_features,))\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(d_model, 1, activation='relu')(inputs)\n",
        "\n",
        "    if use_pe:\n",
        "        x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "        x = PositionalEncoding(n_timesteps, d_model)(x)\n",
        "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    x = EncoderLayer(d_model=d_model, num_heads=nh, dff=_dff, rate=dropout_rate)(x)\n",
        "    x = EncoderLayer(d_model=d_model, num_heads=nh, dff=_dff, rate=dropout_rate)(x)\n",
        "    # x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    x = AttentionWithContext()(x)\n",
        "    # x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(n_outputs * 4, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    # x = tf.keras.layers.Dense(128, activation='relu') (x)\n",
        "\n",
        "    predictions = tf.keras.layers.Dense(n_outputs, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtLfN3hcV3c9",
        "outputId": "bc31455e-be6b-42bc-b0db-1e5768302711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1Sza0LndmZIjGwAh8IAWI16qskTGATgZ9/gesture_recognition/Research code/Transfer learning/AAAI submission\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/gesture_recognition/Research code/Transfer learning/AAAI submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRQyegn_slQ"
      },
      "outputs": [],
      "source": [
        "''' Function to read csv_file '''\n",
        "\n",
        "def read_data(csv_file, column_no, after_column_no):\n",
        "  df = pd.read_csv(csv_file)\n",
        "  if after_column_no==True:\n",
        "    df = df.iloc[: , column_no:]\n",
        "  else:\n",
        "    df = df.iloc[: , :column_no]\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neT4JV_ORJCe"
      },
      "outputs": [],
      "source": [
        "'''Function for random subject-wise splitting'''\n",
        "\n",
        "def leave_subject(curr_df,n=1):\n",
        "  ids =[]\n",
        "  user_ids= np.unique(curr_df['userId'])\n",
        "  for i in range(n):\n",
        "   id = random.choice(user_ids)\n",
        "   ids.append(id)\n",
        "   indices = np.where(user_ids==id)\n",
        "   user_ids = np.delete(user_ids, indices)\n",
        "  test= curr_df[curr_df['userId'].isin(ids)]\n",
        "  train = curr_df[~curr_df['userId'].isin(ids)]\n",
        "  return train,test,ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inADBIlr_zP9"
      },
      "outputs": [],
      "source": [
        "''' Function to prepare data '''\n",
        "\n",
        "import statistics\n",
        "\n",
        "def create_dataset(X, y, time_steps=1, step=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(0, len(X), step):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        labels = y.iloc[i: (i + time_steps)]\n",
        "        Xs.append(v)\n",
        "        ys.append(statistics.mode(labels))\n",
        "\n",
        "    return np.array(Xs), np.array(ys).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHjfjGrERMly"
      },
      "outputs": [],
      "source": [
        "''' Function to scale data '''\n",
        "\n",
        "def scaling_dataframe(df_train, df_val, scale_columns):\n",
        "  scaler = RobustScaler()\n",
        "  scaler = scaler.fit(df_train[scale_columns].values)\n",
        "\n",
        "  df_train.loc[:, scale_columns] = scaler.transform(df_train[scale_columns].to_numpy())\n",
        "  df_val.loc[:, scale_columns] = scaler.transform(df_val[scale_columns].to_numpy())\n",
        "\n",
        "  return df_train, df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DuPQyBdNj0D"
      },
      "outputs": [],
      "source": [
        "'''Custom label encoding function'''\n",
        "\n",
        "def custom_label_encode(data, mapping):\n",
        "    encoded_labels = [mapping[item[0]] for item in data]\n",
        "    return encoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yYJDoN4K2ss"
      },
      "outputs": [],
      "source": [
        "# Read control subject's data from csv\n",
        "df = read_data('control-gesture-data-source.csv', 2, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdoljlCx681x"
      },
      "outputs": [],
      "source": [
        "train_ids = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "gestures_source = ['right', 'left', 'square_C', 'square_AC', 'upRight', 'upLeft', 'rightDown', 'leftDown',\n",
        "                   'v', 'v_Mirror', 'v_Reverse', 'v_ReverseM', 's_Top', 's_TopM', 's_Down', 's_DownM']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1OTfET67pwZ"
      },
      "outputs": [],
      "source": [
        "# df_source = pd.DataFrame(columns=['x_axis', 'y_axis', 'z_axis', 'gesture', 'userId'])\n",
        "# df_source = pd.concat([df_source, df[df['userId'].isin(train_ids) & df['gesture'].isin(gestures_source)]])\n",
        "\n",
        "df_source = pd.DataFrame(columns=df.columns).astype(df.dtypes.to_dict())\n",
        "to_add = df[df['userId'].isin(train_ids) & df['gesture'].isin(gestures_source)]\n",
        "df_source = pd.concat([df_source, to_add], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8x2w9Fv7b0B"
      },
      "outputs": [],
      "source": [
        "df_train, df_val, ids_val = leave_subject(df_source,n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9V5VUw7d6x",
        "outputId": "acfe0d82-9b60-4f56-b47a-db0e4821bf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 5 6 7 8]\n",
            "[4]\n"
          ]
        }
      ],
      "source": [
        "print(df_train.userId.unique())\n",
        "print(df_val.userId.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lBcaxYU7f-m"
      },
      "outputs": [],
      "source": [
        "# Scale data\n",
        "\n",
        "scale_columns = ['x_axis', 'y_axis', 'z_axis']\n",
        "df_train, df_val = scaling_dataframe(df_train, df_val, scale_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGoPUbAE7j4l"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 50 # Block of data to consider a gesture\n",
        "STEP = 50 # Determines overlapping or not\n",
        "\n",
        "X_train, y_train = create_dataset(\n",
        "      df_train[['x_axis', 'y_axis', 'z_axis']],\n",
        "      df_train.gesture,\n",
        "      TIME_STEPS,\n",
        "      STEP\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g1f1zn3kBqY",
        "outputId": "39c54da2-8860-48fa-8f90-908c9bfe24fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2240, 50, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j9cQj4g4Icj"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('pre-trained_Transformer.keras', custom_objects={\"PositionalEncoding\": PositionalEncoding,\n",
        "                                                                                 \"EncoderLayer\": EncoderLayer,\n",
        "                                                                                 \"AttentionWithContext\": AttentionWithContext})\n",
        "intermediate_model = Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "embeddings_control = intermediate_model(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings_control)"
      ],
      "metadata": {
        "id": "m6kak_3OEjHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvpnmmloaXAI"
      },
      "outputs": [],
      "source": [
        "test_ids = [101] # Impaired subject\n",
        "\n",
        "order1_label_mapping = {'circle':0, 'double tap':1, 'rotate_f_s':2, 'rotate_s_f':3, 'shake':4, 'tap':5}\n",
        "\n",
        "# order2_label_mapping = {'rotate_s_f':0, 'tap':1, 'rotate_f_s':2, 'shake':3, 'circle':4, 'double tap':5}\n",
        "\n",
        "# order3_label_mapping = {'double tap':0, 'shake':1, 'rotate_f_s':2, 'circle':3, 'tap':4, 'rotate_s_f':5}\n",
        "\n",
        "# order4_label_mapping = {'rotate_f_s':0, 'tap':1, 'circle':2, 'rotate_s_f':3, 'double tap':4, 'shake':5}\n",
        "\n",
        "# order5_label_mapping = {'shake':0, 'double tap':1, 'rotate_s_f':2, 'tap':3, 'circle':4, 'rotate_f_s':5}\n",
        "\n",
        "df = read_data('impaired-gesture-data-target.csv', 5, False)\n",
        "df_target = pd.DataFrame(columns=df.columns).astype(df.dtypes.to_dict())\n",
        "to_add = df[df['userId'].isin(test_ids) & df['gesture'].isin(['circle', 'double tap', 'rotate_f_s', 'rotate_s_f', 'shake', 'tap'])]\n",
        "df_target = pd.concat([df_target, to_add], ignore_index=True)\n",
        "\n",
        "TIME_STEPS = 50\n",
        "STEP = 50\n",
        "\n",
        "X_target, y_target = create_dataset(\n",
        "      df_target[['x_axis', 'y_axis', 'z_axis']],\n",
        "      df_target.gesture,\n",
        "      TIME_STEPS,\n",
        "      STEP\n",
        "  )\n",
        "\n",
        "y_target = custom_label_encode(y_target, order1_label_mapping)\n",
        "\n",
        "#### Take first 5 samples as train and last 3 samples as test for each gesture ####\n",
        "labels = np.unique(y_target)\n",
        "X_train_target, y_train_target = [], []\n",
        "X_test_target, y_test_target = [], []\n",
        "\n",
        "for label in labels:\n",
        "  c_tr = 0\n",
        "  for ind in range(len(y_target)):\n",
        "    if y_target[ind]==label:\n",
        "      if c_tr<5:\n",
        "        c_tr+=1\n",
        "        X_train_target.append(X_target[ind])\n",
        "        y_train_target.append(y_target[ind])\n",
        "      else:\n",
        "        X_test_target.append(X_target[ind])\n",
        "        y_test_target.append(y_target[ind])\n",
        "\n",
        "###################################################################################\n",
        "\n",
        "X_train_target = np.array(X_train_target)\n",
        "y_train_target = np.array(y_train_target)\n",
        "X_test_target = np.array(X_test_target)\n",
        "y_test_target = np.array(y_test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sryKO7UhrUo6",
        "outputId": "53c7c341-d79e-4336-93bb-bcc8e323a411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 50, 3)\n",
            "(30,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_target.shape)\n",
        "print(y_train_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee1mLM5BbsUJ"
      },
      "outputs": [],
      "source": [
        "''' Function to add gestures one by one for few shot learning '''\n",
        "\n",
        "def add_gestures(X_local, y_local, number_of_gestures):\n",
        "\n",
        "  X_n, y_n = [], []\n",
        "\n",
        "  for i in range(len(y_local)):\n",
        "    if y_local[i] in np.unique(y_local)[:number_of_gestures]:\n",
        "      X_n.append(X_local[i])\n",
        "      y_n.append(y_local[i])\n",
        "\n",
        "  return X_n, y_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMWWmyV4QHtQ"
      },
      "outputs": [],
      "source": [
        "# Cross-attention weights heat map\n",
        "# def plot_attention_weights(attention_weights, iter, epoch, num_class):\n",
        "#   plt.figure(figsize=(16, 12))\n",
        "#   plt.rcParams['font.size'] = 22\n",
        "#   ax = plt.axes()\n",
        "#   sns.heatmap(attention_weights, cmap='viridis')\n",
        "#   plt.xlabel('Source examples')\n",
        "#   plt.ylabel('Target examples')\n",
        "#   # plt.title('Cross-relation Heatmap')\n",
        "#   plt.setp(ax.get_xticklabels(), rotation=90)\n",
        "#   plt.show()\n",
        "#   plt.savefig('Attention-weights-heatmap-class-'+str(num_class)+'-iter-'+str(iter)+'-epoch-'+str(epoch)+'.png')\n",
        "#   plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNVhXC9eHICn"
      },
      "outputs": [],
      "source": [
        "# Attention scores heat map\n",
        "# def plot_attention_scores(attention_scores, iter, epoch, num_class):\n",
        "#   plt.figure(figsize=(16, 12))\n",
        "#   plt.rcParams['font.size'] = 22\n",
        "#   ax = plt.axes()\n",
        "#   sns.heatmap(attention_scores, cmap='viridis')\n",
        "#   plt.xlabel('Feature embeddings')\n",
        "#   plt.ylabel('Training examples')\n",
        "#   # plt.title('Self-example Heatmap')\n",
        "#   # fig.autofmt_xdate()\n",
        "#   plt.setp(ax.get_xticklabels(), rotation=90)\n",
        "#   plt.show()\n",
        "#   plt.savefig('Output-heatmap-class-'+str(num_class)+'-iter-'+str(iter)+'-epoch-'+str(epoch)+'.png')\n",
        "#   plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FubybS2WvFMM"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_loss(embd_one, embd_two, maximize, iter, epoch, num_class, get_embeddings):\n",
        "    if maximize == True:\n",
        "      normalized_embd_one = (embd_one-np.min(embd_one))/(np.max(embd_one)-np.min(embd_one))\n",
        "      mha = MultiHeadAttention(d_model=64, num_heads=4)\n",
        "      attention_scores, attention_weights = mha(normalized_embd_one, embd_one, embd_two, mask=None, return_attention=True)\n",
        "      if get_embeddings:\n",
        "        return attention_scores, attention_weights\n",
        "      # plot_attention_scores(attention_scores, iter, epoch, num_class)\n",
        "      # plot_attention_weights(attention_weights, iter, epoch, num_class)\n",
        "      attention_scores = np.array(attention_scores)\n",
        "      attention_scores = attention_scores[attention_scores > 0]\n",
        "      normalized_scores = (attention_scores-np.min(attention_scores))/(np.max(attention_scores)-np.min(attention_scores))\n",
        "      loss = 1.0 - np.mean(normalized_scores)\n",
        "    else:\n",
        "      embd_one_flat = np.hstack(embd_one)\n",
        "      embd_two_flat = np.hstack(embd_two)\n",
        "      cos_sim = 1.0 - spatial.distance.cosine(embd_one_flat, embd_two_flat)\n",
        "      loss = cos_sim\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQorVP7PdNsm"
      },
      "outputs": [],
      "source": [
        "def classification_loss(y_true, y_pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    # print(\"Classification Loss: {}\".format(loss_object(y_true, y_pred)))\n",
        "\n",
        "    return loss_object(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uWuXyPBekuu"
      },
      "outputs": [],
      "source": [
        "def overall_loss(model, inputs, targets, embdc, embdif, embdi, it, ep, num_class, get_embeddings):\n",
        "  alpha = 0.25    # 0.25\n",
        "  beta = 0.25     # 0.25\n",
        "  gamma = 0.5     # 0.5\n",
        "  loss_con_imp = cosine_similarity_loss(embdc, embdi, True, it, ep, num_class, get_embeddings)\n",
        "  loss_imp_imp = cosine_similarity_loss(embdif, embdi, False, it, ep, num_class, get_embeddings)\n",
        "  loss_cls = classification_loss(targets, model(inputs, training=True))\n",
        "  total_loss = (alpha*loss_con_imp) + (beta*loss_imp_imp) + (gamma*loss_cls)\n",
        "\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ6NgtbXTZPJ"
      },
      "outputs": [],
      "source": [
        "def grad(model, inputs, targets, embdc, embdif, embdi, it, ep, num_class, get_embeddings):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = overall_loss(model, inputs, targets, embdc, embdif, embdi, it, ep, num_class, get_embeddings)\n",
        "  return tape.gradient(loss_value, model.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5ZflQNkTvng"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "em_YhgxtI5hF"
      },
      "outputs": [],
      "source": [
        "csvfile_prf = open('avg_metric_score_gesture.csv', 'w', encoding='utf-8')\n",
        "csvfile_writer_prf = csv.writer(csvfile_prf)\n",
        "csvfile_writer_prf.writerow([\"number of samples\", \"gesture\", \"avg_precision\", \"avg_recall\", \"avg_f1_score\"])\n",
        "\n",
        "csvfile_acc = open('avg_acc_gesture.csv', 'w', encoding='utf-8')\n",
        "csvfile_writer_acc = csv.writer(csvfile_acc)\n",
        "csvfile_writer_acc.writerow([\"number of samples\", \"number of gestures\", \"avg_accuracy\"])\n",
        "\n",
        "cm = defaultdict(list)\n",
        "\n",
        "number_of_gestures = 6 # Original - 6 gestures\n",
        "start = 2 # Initilialize with two gestures\n",
        "\n",
        "for cur in range(start, number_of_gestures+1): # Iterate over the gestures. Starts from 0,1.\n",
        "\n",
        "  copy_model = keras.models.load_model('pre-trained_Transformer.keras', custom_objects={\"PositionalEncoding\": PositionalEncoding,\n",
        "                                                                                 \"EncoderLayer\": EncoderLayer,\n",
        "                                                                                 \"AttentionWithContext\": AttentionWithContext})\n",
        "  intermediate_copy_model = Model(inputs=copy_model.input, outputs=copy_model.layers[-3].output)\n",
        "\n",
        "  gesture_labels = [i for i in range(cur)] # gesture labels in order\n",
        "\n",
        "  cls_ges_pre = defaultdict(list)\n",
        "  cls_ges_rec = defaultdict(list)\n",
        "  cls_ges_f1 = defaultdict(list)\n",
        "\n",
        "  count = 1\n",
        "\n",
        "  X_train_n, y_train_n = add_gestures(X_train_target, y_train_target, cur)\n",
        "  X_test_n, y_test_n = add_gestures(X_test_target, y_test_target, cur)\n",
        "\n",
        "  target_names = list(order1_label_mapping.keys())[:cur]\n",
        "\n",
        "  while (count): # Adding 1 example, then 3 examples and then 5 examples.\n",
        "    X_train_f = []\n",
        "    y_train_f = []\n",
        "    c={}\n",
        "    for i in gesture_labels:\n",
        "      c[i]=0\n",
        "    for l in range(len(y_train_n)):\n",
        "      if(c[y_train_n[l]]!=count):\n",
        "        X_train_f.append(X_train_n[l])\n",
        "        y_train_f.append(y_train_n[l])\n",
        "        c[y_train_n[l]]+=1\n",
        "\n",
        "    X_train_n_arr = np.array(X_train_f) # Converting list to array (train set)\n",
        "    y_train_n_arr = np.array(y_train_f)\n",
        "\n",
        "    X_test_n_arr = np.array(X_test_n) # Converting list to array (test set)\n",
        "    y_test_n_arr = np.array(y_test_n)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    X_train_n_arr = scaler.fit_transform(X_train_n_arr.reshape(-1, X_train_n_arr.shape[-1])).reshape(X_train_n_arr.shape)\n",
        "    X_test_n_arr = scaler.transform(X_test_n_arr.reshape(-1, X_test_n_arr.shape[-1])).reshape(X_test_n_arr.shape)\n",
        "\n",
        "    # Fixed embeddings representation for impaired samples\n",
        "    embeddings_impaired_fixed = intermediate_copy_model(X_train_n_arr)\n",
        "\n",
        "    results_fixed_embedding = {'embedding-fixed': np.array(embeddings_impaired).tolist()}\n",
        "    file_name = f\"embedding_fixed-{cur}-classes-{count}-examples.josn\"\n",
        "    with open(file_name, \"w\") as f:\n",
        "        json.dump(results_fixed_embedding, f)\n",
        "\n",
        "\n",
        "    num_epochs = 15\n",
        "\n",
        "  #######################################################\n",
        "    ''' Fine-tuning with KARL '''\n",
        "  #######################################################\n",
        "\n",
        "    cls_ges_sum_pre = defaultdict(list)\n",
        "    cls_ges_sum_rec = defaultdict(list)\n",
        "    cls_ges_sum_f1 = defaultdict(list)\n",
        "    cls_acc = []\n",
        "\n",
        "    output_file = \"all_embeddings-\"+str(cur)+\"-classes-\"+str(count)+\"-examples\"+\".json\"\n",
        "    results = {}\n",
        "    for iter in range(10):\n",
        "      train_model = create_model(50, 3, 16, d_model=128, nh=4, dropout_rate=0.2)\n",
        "      train_model.load_weights('pre-trained_Transformer.keras')\n",
        "      layers = train_model.layers\n",
        "      new_output = layers[-2].output\n",
        "      new_layer = Dense(len(gesture_labels), activation='softmax')(new_output)\n",
        "      train_model = Model(inputs=train_model.input, outputs=new_layer)\n",
        "      for layer in train_model.layers[:-3]:\n",
        "        layer.trainable = False\n",
        "      # train_model.summary()\n",
        "\n",
        "      iteration_key = f\"iteration_{iter+1}\"\n",
        "      results[iteration_key] = {}\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "        intermediate_train_model = Model(inputs=train_model.input, outputs=train_model.layers[-3].output)\n",
        "        embeddings_impaired = intermediate_train_model(X_train_n_arr)\n",
        "\n",
        "        # Storing attention scores and weights in json during training\n",
        "        attention_scores, attention_weights = cosine_similarity_loss(embeddings_control, embeddings_impaired, True, iter, epoch, cur, True)\n",
        "        results[iteration_key][f\"epoch_{epoch+1}\"] = {\n",
        "            \"attention_scores\": np.array(attention_scores).tolist(),\n",
        "            \"attention_weights\": np.array(attention_weights).tolist()\n",
        "        }\n",
        "\n",
        "        grads = grad(train_model, X_train_n_arr, y_train_n_arr, embeddings_control, embeddings_impaired_fixed, embeddings_impaired, iter, epoch, cur, False)\n",
        "        optimizer.apply_gradients(zip(grads, train_model.trainable_variables))\n",
        "\n",
        "      y_pred = np.argmax(train_model.predict(X_test_n_arr), axis=-1)\n",
        "      cm[cur].append(confusion_matrix(y_test_n_arr, y_pred))\n",
        "\n",
        "      report = classification_report(y_test_n_arr, y_pred, target_names=target_names)\n",
        "      # Store all ierations' precision, recall and F1-score for each gesture from the classification report\n",
        "      for item in target_names:\n",
        "        precision = float(report.split(item)[1].split()[0])\n",
        "        recall = float(report.split(item)[1].split()[1])\n",
        "        f1_score = float(report.split(item)[1].split()[2])\n",
        "\n",
        "        cls_ges_sum_pre[item].append(precision)\n",
        "        cls_ges_sum_rec[item].append(recall)\n",
        "        cls_ges_sum_f1[item].append(f1_score)\n",
        "\n",
        "      cls_acc.append(float(report.split('accuracy')[1].split()[0]))\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "      json.dump(results, f, indent=2)\n",
        "\n",
        "    # Store the average precision, recall and F1-score for each gesture\n",
        "    for n in target_names:\n",
        "      cls_ges_pre[n].append(np.mean(cls_ges_sum_pre[n]))\n",
        "      cls_ges_rec[n].append(np.mean(cls_ges_sum_rec[n]))\n",
        "      cls_ges_f1[n].append(np.mean(cls_ges_sum_f1[n]))\n",
        "\n",
        "      avg_pre = np.mean(cls_ges_sum_pre[n])\n",
        "      avg_rec = np.mean(cls_ges_sum_rec[n])\n",
        "      avg_f1 = np.mean(cls_ges_sum_f1[n])\n",
        "\n",
        "      csv_line_prf = [count, n, avg_pre, avg_rec, avg_f1]\n",
        "      csvfile_writer_prf.writerow(csv_line_prf)\n",
        "\n",
        "    csv_line_acc = [count, len(gesture_labels), np.mean(cls_acc)]\n",
        "    csvfile_writer_acc.writerow(csv_line_acc)\n",
        "\n",
        "    count+=2\n",
        "    if(count>5):\n",
        "      break\n",
        "\n",
        "csvfile_prf.close()\n",
        "csvfile_acc.close()\n",
        "\n",
        "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15R1mHOMFUVf"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  for key, value in cm.items():\n",
        "    cm[key] = [np.array(arr).tolist() for arr in value]\n",
        "  with open('cm_file_order5_karl_114.json', 'w') as f:\n",
        "    json.dump(cm, f)\n",
        "\n",
        "except:\n",
        "  print(\"Not able to save!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RciKAsdw4W-Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representation analysis"
      ],
      "metadata": {
        "id": "_BZEk1rLe77g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PnktuqC4W7g"
      },
      "outputs": [],
      "source": [
        "with open(\"all_embeddings-3-classes-3-examples.json\", \"r\") as f:\n",
        "  results = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  t-SNE implementation\n",
        "'''\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter\n",
        "\n",
        "number_of_classes = 3\n",
        "number_of_examples = 3\n",
        "\n",
        "attention_weights = results['iteration_1']['epoch_15']['attention_weights']\n",
        "attention_scores = results['iteration_1']['epoch_15']['attention_scores']\n",
        "\n",
        "gestures = ['circle', 'double tap', 'fast outward, slow inward']\n",
        "\n",
        "k = 224 # 10% of source dataset\n",
        "top_indices = [np.argsort(row)[-k:][::-1].tolist() for row in attention_weights]\n",
        "\n",
        "source_embeddings_list = []\n",
        "for index in top_indices:\n",
        "  source_samples = X_train[index]\n",
        "  source_labels = y_train[index]\n",
        "  counts = Counter(source_labels.flatten())\n",
        "  print(counts)\n",
        "  embedding = intermediate_model(source_samples)\n",
        "  source_embeddings_list.append(embedding)\n",
        "\n",
        "# For each sample, get top 2 classes and their embeddings\n",
        "all_sample_embeddings = []\n",
        "all_sample_labels = []\n",
        "\n",
        "for sample_idx in range(len(attention_weights)):\n",
        "    # Get indices sorted by attention weights for this sample\n",
        "    sorted_indices = np.argsort(attention_weights[sample_idx])[::-1] # length -> 2240\n",
        "\n",
        "    # Get top k indices\n",
        "    top_k_indices = sorted_indices[:k] # 224\n",
        "    top_k_labels = y_train[top_k_indices].flatten() # 224\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    class_counts = Counter(top_k_labels)\n",
        "\n",
        "    # Get top 2 classes by count\n",
        "    top_2_classes = [cls for cls, count in class_counts.most_common(2)]\n",
        "    print(top_2_classes)\n",
        "    # Get embeddings for top 2 classes\n",
        "    for idx in top_k_indices:\n",
        "        if y_train[idx] in top_2_classes:\n",
        "            all_sample_embeddings.append(source_embeddings_list[sample_idx][np.where(top_k_indices == idx)[0][0]])\n",
        "            all_sample_labels.append(y_train[idx].item())\n",
        "\n",
        "all_sample_embeddings = np.vstack(all_sample_embeddings)\n",
        "all_sample_labels = np.vstack(all_sample_labels)\n",
        "all_sample_labels = [label[0] for label in all_sample_labels]\n",
        "\n",
        "print(len(all_sample_embeddings))\n",
        "\n",
        "# Apply t-SNE to source examples\n",
        "tsne1 = TSNE(n_components=2, perplexity=40, n_iter=1000, random_state=42)\n",
        "components_weights = tsne1.fit_transform(all_sample_embeddings)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.rcParams['font.size'] = 18\n",
        "\n",
        "colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'lime', 'fuchsia', 'lawngreen', 'indigo', 'gold', 'deepskyblue', 'teal', 'crimson']\n",
        "unique_classes = np.unique(all_sample_labels)\n",
        "\n",
        "for class_id, class_name in enumerate(unique_classes):\n",
        "    mask = []\n",
        "    for idx, label in enumerate(all_sample_labels):\n",
        "      mask.append(True) if label == class_name else mask.append(False)\n",
        "\n",
        "    plt.scatter(components_weights[mask, 0], components_weights[mask, 1], s=50,\n",
        "                  color=colors[class_id], label=f'{class_name}', alpha=0.7)\n",
        "\n",
        "labels = []\n",
        "for class_id in range(number_of_classes):\n",
        "  labels.extend([class_id] * number_of_examples)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Apply t-SNE to target examples\n",
        "tsne2 = TSNE(n_components=2, perplexity=2, n_iter=1000, random_state=42)\n",
        "components_scores = tsne2.fit_transform(np.array(attention_scores))  # (9, 2)\n",
        "\n",
        "colors = ['black', 'orange', 'brown']\n",
        "markers=['s', 'p', '*']\n",
        "for class_id, color in enumerate(colors):\n",
        "    mask = labels == class_id\n",
        "    print(mask)\n",
        "    print(components_scores[mask, 0], components_scores[mask, 1])\n",
        "    plt.scatter(components_scores[mask, 0], components_scores[mask, 1], s=300,\n",
        "                color=color, marker=markers[class_id] ,label=f'{gestures[class_id]}', alpha=0.7)\n",
        "\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE plot')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('tSNE_of_Source_Target_Embeddings_final.png')\n",
        "plt.show()\n",
        "\n",
        "print(\"t-SNE visualization completed\")"
      ],
      "metadata": {
        "id": "z45Ra0oYteIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YleV5fRbtdr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}